{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6fce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras_tuner as kt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers, backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaa7752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f1224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "train_df = pd.read_csv(\"train_dataset.csv\")\n",
    "test_df = pd.read_csv(\"test_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "718897fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data into feature and response variables\n",
    "X_train = train_df.drop(columns=[\"TYPE\"])\n",
    "y_train = train_df[\"TYPE\"]\n",
    "X_test = test_df.drop(columns=[\"TYPE\"])\n",
    "y_test = test_df[\"TYPE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "388d84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables using One-Hot Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_cols = train_df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "X_train_encoded = encoder.fit_transform(X_train[categorical_cols])\n",
    "X_test_encoded = encoder.transform(X_test[categorical_cols])\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(categorical_cols), index=X_train.index)\n",
    "X_test_encoded = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(categorical_cols), index=X_test.index)\n",
    "\n",
    "# Drop original categorical columns and concatenate encoded ones\n",
    "X_train = X_train.drop(columns=categorical_cols).reset_index(drop=True).join(X_train_encoded)\n",
    "X_test = X_test.drop(columns=categorical_cols).reset_index(drop=True).join(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b1a6c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Further split training data into training + validation\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbf0a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build neural network model\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=(X_train_final.shape[1],)))\n",
    "\n",
    "    # Tune the number of layers and units. We try 1-3 hidden layers with different number of neurons in each hidden layer\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=16, max_value=64, step=16),\n",
    "            activation='relu',\n",
    "        ))\n",
    "        model.add(layers.Dropout(hp.Float(f'dropout_{i}', 0.1, 0.5, step=0.1)))  # Apply dropout regularization\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Output layer\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice('learning_rate', [1e-3, 5e-4, 1e-4])\n",
    "        ),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "472803bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create keras_tuner object that is used for hyperparameter tuning\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=kt.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78b5389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 58s]\n",
      "val_accuracy: 0.8913569450378418\n",
      "\n",
      "Best val_accuracy So Far: 0.94917893409729\n",
      "Total elapsed time: 00h 07m 54s\n"
     ]
    }
   ],
   "source": [
    "# Early stopping condition\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Search for best hyperparameters\n",
    "tuner.search(X_train_final, y_train_final,\n",
    "             validation_data=(X_val, y_val),\n",
    "             epochs=100,\n",
    "             batch_size=32,\n",
    "             callbacks=[early_stop],\n",
    "             verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbc016aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'num_layers': 2, 'units_0': 64, 'dropout_0': 0.5, 'learning_rate': 0.001, 'units_1': 32, 'dropout_1': 0.30000000000000004, 'units_2': 16, 'dropout_2': 0.30000000000000004}\n"
     ]
    }
   ],
   "source": [
    "# Get the best trial from the tuner\n",
    "best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "\n",
    "# Print the best hyperparameters for that trial\n",
    "print(\"Best Hyperparameters:\", best_trial.hyperparameters.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce528e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287us/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0376    0.3000    0.0668        60\n",
      "           1     0.9938    0.9363    0.9642      7232\n",
      "\n",
      "    accuracy                         0.9310      7292\n",
      "   macro avg     0.5157    0.6181    0.5155      7292\n",
      "weighted avg     0.9860    0.9310    0.9568      7292\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  18   42]\n",
      " [ 461 6771]]\n"
     ]
    }
   ],
   "source": [
    "# Use the best_model for prediction\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_proba = best_model.predict(X_test)\n",
    "\n",
    "# Final evaluation\n",
    "y_pred_final = (y_pred_proba >= 0.5).astype(int)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_final,digits=4))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_final))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a170e",
   "metadata": {},
   "source": [
    "Remarks on reproducibility: Due to the randomness of the training process, the optimal hyperparameters chosen in each iteration of running the code may be different. However, the results are generally consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49611c4a",
   "metadata": {},
   "source": [
    "Here, we implement a feedforward neural network. After hyperparameter selection, the chosen architecture is:\n",
    "\n",
    "Input layer: 17 units (Corresponding to 17 features in the dataset)\n",
    "\n",
    "Hidden Layers: 2\n",
    "\n",
    "1st hidden layer: 64 units with 0.5 dropout regularisation\n",
    "\n",
    "2nd hidden layer: 32 units with 0.3 dropout regularisation\n",
    "\n",
    "Dropout regularisation is performed to prevent the model from overfitting on the train data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f38de",
   "metadata": {},
   "source": [
    "Looking at the F1-scores for \"Bad\" clients, it has a very low score of 0.0668, indicating that the model struggles significantly with predicting \"Bad\" clients. Given this poor performance, it suggests that a neural network might not be the most suitable approach for this problem, likely due to its limitations in handling imbalanced datasets. Other methods such as logistic regression, random forests, and gradient boosting machines could provide better results, as these models tend to be more adept at capturing the complexities of imbalanced data and may offer improved accuracy for both classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
